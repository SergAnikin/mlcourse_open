{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 2\n",
    "Авторы материала: Павел Нестеров. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/1I_ticU8rpeoGJjsBUcaInpvgdxdq60hV7IcSvo4rlGo/).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн версию алгоритма мультиклассовой классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = 'data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'javascript', 'python', 'java', 'android', 'php', 'c++', 'c#', 'jquery', 'ios', 'html'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции, и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. <font color='green'>$\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$</font>\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. <font color='green'>$\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$</font>\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Имплементация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$ если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = -800\n",
    "1. / (1. + np.exp(- s)) if np.exp(s) != 0 else 0. # мой код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    top_tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы создаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag] # мой код\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]] # мой код\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = 1. / (1. + np.exp(- z)) if np.exp(z) != 0 else 0. # мой код\n",
    "                   \n",
    "                            \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += - ((y * np.log(sigma) + (1 - y) * np.log(1 - sigma))\n",
    "                                      if sigma > tolerance and 1 - sigma > tolerance \n",
    "                                      else \n",
    "                                      (y * np.log(tolerance) + (1 - y) * np.log(1 - sigma)) \n",
    "                                      if sigma < tolerance\n",
    "                                      else \n",
    "                                      (y * np.log(sigma) + (1 - y) * np.log(1 - tolerance))\n",
    "                                     )\n",
    "                    \n",
    "#                     sample_loss += - ((y * np.log(sigma) + (1 - y) * np.log(1 - sigma))\n",
    "#                                       if sigma > tolerance and 1 - sigma > tolerance \n",
    "#                                       else \n",
    "#                                       ((1 - y) * np.log(1 - sigma)) \n",
    "#                                       if sigma < tolerance\n",
    "#                                       else \n",
    "#                                       (y * np.log(sigma)))\n",
    "                    # мой код\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma)\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ef760cfe084ca69a5b7476e45ae2a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 000 примеров, чтобы хоть как то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA50AAAKqCAYAAAC5JDrrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAF5D0AkAAAAA8BqCTgAAAACA1/w/hkOBpFTzX/4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fb5897ebe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 13.58\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической точности вы используем грязный трюк: мы будем регуляризаровать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение не регуляризируется. `sample_loss` тоже должен остаться без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet регуляризация, имплементация\n",
    "\n",
    "В качестве седьмой задачи, вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной ElasticNet регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что неудивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. java, c#\n",
    "2. php, javascript\n",
    "3. html, jquery\n",
    "4. ios, android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре - 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение сегодняшней домашки, вам предлагается реализовать метод `predict_proba`, который принимает строку,  содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, какой или какие теги ассоциируются с данным вопросом, если порог принятия равен $0.9$?:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. ios, php\n",
    "4. c#, c++, ods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
